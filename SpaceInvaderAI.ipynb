{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2775dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off of the original Space Invader AI tutorial by Nicholas Renotte\n",
    "# URL Link: https://github.com/nicknochnack/KerasRL-OpenAI-Atari-SpaceInvadersv0/blob/main/Space%20Invaders%20Walkthrough.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598029af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.3.1 in /home/maxbrette/.local/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: gym in /home/maxbrette/.local/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: keras-rl2 in /home/maxbrette/.local/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (2.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==2.3.1) (0.34.2)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (0.3.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.3.1) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (3.19.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (1.13.3)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (1.6.3)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (2.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (3.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (1.43.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorflow==2.3.1) (2.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/maxbrette/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (45.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.22.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/maxbrette/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/maxbrette/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.10.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/maxbrette/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "!pip install tensorflow==2.3.1 gym keras-rl2 gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b37580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: gym 0.19.0\n",
      "Uninstalling gym-0.19.0:\n",
      "  Would remove:\n",
      "    /home/maxbrette/.local/lib/python3.8/site-packages/gym-0.19.0.dist-info/*\n",
      "    /home/maxbrette/.local/lib/python3.8/site-packages/gym/*\n",
      "  Would not remove (might be manually added):\n",
      "    /home/maxbrette/.local/lib/python3.8/site-packages/gym/envs/atari/environment.py\n",
      "Proceed (y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install gym[atari]\n",
    "#!pip uninstall gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecf814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install atari-py\n",
    "#!pip uninstall atari-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13aad7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[accept-rom-license] in /home/maxbrette/.local/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from gym[accept-rom-license]) (1.18.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/maxbrette/.local/lib/python3.8/site-packages (from gym[accept-rom-license]) (2.0.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\" in /home/maxbrette/.local/lib/python3.8/site-packages (from gym[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: click in /home/maxbrette/.local/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (8.0.3)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /home/maxbrette/.local/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (5.4.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.22.0)\n",
      "Requirement already satisfied: tqdm in /home/maxbrette/.local/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (4.62.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license; extra == \"accept-rom-license\" in /home/maxbrette/.local/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/maxbrette/.local/lib/python3.8/site-packages (from importlib-resources; python_version < \"3.9\"->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b433308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyglet\n",
      "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyglet\n",
      "Successfully installed pyglet-1.5.21\n"
     ]
    }
   ],
   "source": [
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55f418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#Test Random Environment with OpenAI Gym\n",
    "import gym\n",
    "import random\n",
    "#import atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9d9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(atari_py.list_games())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "489fa7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5c5359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b9837ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxbrette/.local/lib/python3.8/site-packages/gym/envs/atari/environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 135.0\n",
      "Episode: 2 Score: 110.0\n",
      "Episode: 3 Score: 210.0\n",
      "Episode: 4 Score: 95.0\n",
      "Episode: 5 Score: 80.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes + 1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0, 1, 2, 3, 4, 5])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode: {} Score: {}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9183a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 15:27:33.247853: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2022-02-08 15:27:33.247880: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Create a Deep Learning Model With Keras\n",
    "import numpy as no\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "223cb7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides = (4,4), activation = 'relu', \n",
    "                            input_shape = (3, height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides = (2,2), activation = 'relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation = 'relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation = 'relu'))\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dense(actions, activation = 'linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "537ba81a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96a8af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aae7d6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e599122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Agenth With Keras-RL\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdb0c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                                  attr = 'eps', \n",
    "                                  value_max = 1., \n",
    "                                  value_min = .1, \n",
    "                                  value_test = .2, \n",
    "                                  nb_steps = 10000)\n",
    "    \n",
    "    memory = SequentialMemory(limit = 1000, \n",
    "                              window_length = 3)\n",
    "    \n",
    "    dqn = DQNAgent(model = model, \n",
    "                   memory = memory, \n",
    "                   policy = policy, \n",
    "                   enable_dueling_network = True, \n",
    "                   dueling_type = 'avg', \n",
    "                   nb_actions = actions, \n",
    "                   nb_steps_warmup = 1000)\n",
    "    \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5d72164",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr = 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c16138d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "WARNING:tensorflow:From /home/maxbrette/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      " 1015/10000: episode: 1, duration: 56.502s, episode steps: 1015, steps per second:  18, episode reward: 255.000, mean reward:  0.251 [ 0.000, 30.000], mean action: 2.533 [0.000, 5.000],  loss: 308.787573, mean_q: 22.177942, mean_eps: 0.909325\n",
      " 1519/10000: episode: 2, duration: 722.811s, episode steps: 504, steps per second:   1, episode reward: 55.000, mean reward:  0.109 [ 0.000, 20.000], mean action: 2.458 [0.000, 5.000],  loss: 2.004889, mean_q: 12.254759, mean_eps: 0.886015\n",
      " 2671/10000: episode: 3, duration: 1533.417s, episode steps: 1152, steps per second:   1, episode reward: 300.000, mean reward:  0.260 [ 0.000, 30.000], mean action: 2.486 [0.000, 5.000],  loss: 1.084927, mean_q: 11.563277, mean_eps: 0.811495\n",
      " 3788/10000: episode: 4, duration: 1491.607s, episode steps: 1117, steps per second:   1, episode reward: 220.000, mean reward:  0.197 [ 0.000, 30.000], mean action: 2.492 [0.000, 5.000],  loss: 0.632223, mean_q: 10.947349, mean_eps: 0.709390\n",
      " 4592/10000: episode: 5, duration: 1073.814s, episode steps: 804, steps per second:   1, episode reward: 195.000, mean reward:  0.243 [ 0.000, 25.000], mean action: 2.570 [0.000, 5.000],  loss: 0.574146, mean_q: 11.166124, mean_eps: 0.622945\n",
      " 5422/10000: episode: 6, duration: 1103.398s, episode steps: 830, steps per second:   1, episode reward: 180.000, mean reward:  0.217 [ 0.000, 30.000], mean action: 2.433 [0.000, 5.000],  loss: 0.527945, mean_q: 11.758616, mean_eps: 0.549415\n",
      " 5861/10000: episode: 7, duration: 571.304s, episode steps: 439, steps per second:   1, episode reward: 45.000, mean reward:  0.103 [ 0.000, 25.000], mean action: 2.326 [0.000, 5.000],  loss: 0.473015, mean_q: 11.701825, mean_eps: 0.492310\n",
      " 6336/10000: episode: 8, duration: 614.998s, episode steps: 475, steps per second:   1, episode reward: 35.000, mean reward:  0.074 [ 0.000, 15.000], mean action: 2.272 [0.000, 5.000],  loss: 0.215459, mean_q: 11.331537, mean_eps: 0.451180\n",
      " 7171/10000: episode: 9, duration: 1094.186s, episode steps: 835, steps per second:   1, episode reward: 265.000, mean reward:  0.317 [ 0.000, 30.000], mean action: 2.444 [0.000, 5.000],  loss: 0.366943, mean_q: 11.746981, mean_eps: 0.392230\n",
      " 7791/10000: episode: 10, duration: 827.506s, episode steps: 620, steps per second:   1, episode reward: 150.000, mean reward:  0.242 [ 0.000, 30.000], mean action: 2.324 [0.000, 5.000],  loss: 0.417050, mean_q: 11.629385, mean_eps: 0.326755\n",
      " 8465/10000: episode: 11, duration: 897.834s, episode steps: 674, steps per second:   1, episode reward: 65.000, mean reward:  0.096 [ 0.000, 15.000], mean action: 2.399 [0.000, 5.000],  loss: 0.295564, mean_q: 11.231945, mean_eps: 0.268525\n",
      " 9110/10000: episode: 12, duration: 864.200s, episode steps: 645, steps per second:   1, episode reward: 130.000, mean reward:  0.202 [ 0.000, 25.000], mean action: 2.315 [0.000, 5.000],  loss: 0.191953, mean_q: 11.760819, mean_eps: 0.209170\n",
      " 9768/10000: episode: 13, duration: 879.141s, episode steps: 658, steps per second:   1, episode reward: 210.000, mean reward:  0.319 [ 0.000, 30.000], mean action: 2.660 [0.000, 5.000],  loss: 0.386169, mean_q: 11.631514, mean_eps: 0.150535\n",
      "done, took 12040.121 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbfc5be55e0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps = 10000, visualize = False, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea7a72a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 205.000, steps: 938\n",
      "Episode 2: reward: 170.000, steps: 719\n",
      "Episode 3: reward: 195.000, steps: 693\n",
      "Episode 4: reward: 75.000, steps: 668\n",
      "Episode 5: reward: 120.000, steps: 755\n",
      "Episode 6: reward: 110.000, steps: 486\n",
      "Episode 7: reward: 195.000, steps: 788\n",
      "Episode 8: reward: 5.000, steps: 395\n",
      "Episode 9: reward: 165.000, steps: 704\n",
      "Episode 10: reward: 225.000, steps: 1207\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m scores \u001b[38;5;241m=\u001b[39m dqn\u001b[38;5;241m.\u001b[39mtest(env, nb_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, visualize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mmean(scores\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes = 10, visualize = True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32172fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading Agent From Memory\n",
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weightss.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b008d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3bf06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weightss.h5f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
